{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 23:00:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:11:00.0  On |                  N/A |\n",
      "| 37%   49C    P5              42W / 350W |    519MiB / 24576MiB |     38%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1945      G   /usr/lib/xorg/Xorg                          159MiB |\n",
      "|    0   N/A  N/A      2148      G   /usr/bin/gnome-shell                         43MiB |\n",
      "|    0   N/A  N/A      3581      G   ...62,262144 --variations-seed-version      119MiB |\n",
      "|    0   N/A  N/A      4680      G   ...erProcess --variations-seed-version      166MiB |\n",
      "|    0   N/A  N/A      5902      G   /usr/bin/nautilus                            15MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 23.38 GB\n",
      "Available Memory: 17.89 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the total and available memory\n",
    "memory_info = psutil.virtual_memory()\n",
    "\n",
    "total_memory = memory_info.total / (1024 ** 3)  # Convert from bytes to GB\n",
    "available_memory = memory_info.available / (1024 ** 3)  # Convert from bytes to GB\n",
    "\n",
    "print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/home/firqaaa/anaconda3/envs/Research/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdb72ee8f6f46a499a25f220478a728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :\n",
      "Real/Spoof : Spoof\n",
      "Attack Type : Mask (silicone)\n",
      "Explanation : The image shows signs of being a spoof attack. The face appears unnaturally smooth and lacks the natural texture and contours of a real human face. Additionally, the edges around the face and the overall appearance suggest that a mask made of silicone or a similar material has been used to spoof the image.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model configurations\n",
    "config = PeftConfig.from_pretrained(\"firqaaa/vsft-llava-1.5-7b-hf-liveness-trl\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\",\n",
    "                                                               torch_dtype=torch.float16,\n",
    "                                                               low_cpu_mem_usage=True,\n",
    "                                                               device_map=\"auto\",\n",
    "                                                               load_in_4bit=True,\n",
    "                                                               attn_implementation=\"flash_attention_2\")\n",
    "model = PeftModel.from_pretrained(base_model, \"firqaaa/vsft-llava-1.5-7b-hf-liveness-trl\")\n",
    "model.to(device)\n",
    "\n",
    "# url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_path = \"/home/firqaaa/Python/Retired-Yann-LeCun/silicone_frames/5/frame7.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "prompt = \"\"\"USER: <image>\\nI ask you to be an liveness image annotator expert to determine if an image \"Real\" or \"Spoof\". \n",
    "If an image is a \"Spoof\" define what kind of attack, is it spoofing attack that used Print(flat), Replay(monitor, laptop), or Mask(paper, crop-paper, silicone)?\n",
    "If an image is a \"Real\" or \"Normal\" return \"No Attack\". \n",
    "Whether if an image is \"Real\" or \"Spoof\" give an explanation to this.\n",
    "Return your response using following format :\n",
    "\n",
    "Real/Spoof : \n",
    "Attack Type :\n",
    "Explanation :\\nASSISTANT:\"\"\"\n",
    "\n",
    "# Prepare inputs and move to device\n",
    "inputs = processor(prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(\"Response :\")\n",
    "# Decode and print the output\n",
    "decoded_output = processor.decode(output[0], skip_special_tokens=True).split(\"ASSISTANT:\")[-1].strip()\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
